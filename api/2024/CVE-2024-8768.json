{
  "mitigation": "Red Hat has investigated whether a possible mitigation exists for this issue, and has not been able to identify a practical example. Please update the affected package as soon as possible.",
  "affected_release": null,
  "package_state": [
    {
      "product_name": "Red Hat Enterprise Linux AI (RHEL AI)",
      "fix_state": "Affected",
      "package_name": "rhelai1/bootc-nvidia-rhel9",
      "cpe": "cpe:/a:redhat:enterprise_linux_ai:1"
    },
    {
      "product_name": "Red Hat Enterprise Linux AI (RHEL AI)",
      "fix_state": "Affected",
      "package_name": "rhelai1/instructlab-nvidia-rhel9",
      "cpe": "cpe:/a:redhat:enterprise_linux_ai:1"
    }
  ],
  "threat_severity": "Important",
  "public_date": "2024-08-22T12:00:00Z",
  "bugzilla": {
    "description": "vllm: A completions API request with an empty prompt will crash the vllm API server.",
    "id": "2311895",
    "url": "https://bugzilla.redhat.com/show_bug.cgi?id=2311895"
  },
  "cvss": {
    "cvss_base_score": "",
    "cvss_scoring_vector": "",
    "status": ""
  },
  "cvss3": {
    "cvss3_base_score": "7.5",
    "cvss3_scoring_vector": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H",
    "status": "draft"
  },
  "iava": "",
  "cwe": "CWE-617",
  "statement": "This vulnerability can only be exploited when vLLM is serving GPT-2 models, other models are not affected by this issue.\nAs this flaw allows remote users to cause a denial of service, it has been rated with an important severity.",
  "acknowledgement": "",
  "name": "CVE-2024-8768",
  "document_distribution": "",
  "details": [
    "A flaw was found in the vLLM library. A completions API request with an empty prompt will crash the vLLM API server, resulting in a denial of service.",
    "A flaw was found in the vLLM library. A completions API request with an empty prompt will crash the vLLM API server, resulting in a denial of service."
  ],
  "references": [
    "https://www.cve.org/CVERecord?id=CVE-2024-8768\nhttps://nvd.nist.gov/vuln/detail/CVE-2024-8768\nhttps://github.com/vllm-project/vllm/issues/7632\nhttps://github.com/vllm-project/vllm/pull/7746"
  ]
}
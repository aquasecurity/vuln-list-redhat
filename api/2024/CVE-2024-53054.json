{
  "affected_release": null,
  "package_state": [
    {
      "product_name": "Red Hat Enterprise Linux 6",
      "fix_state": "Not affected",
      "package_name": "kernel",
      "cpe": "cpe:/o:redhat:enterprise_linux:6"
    },
    {
      "product_name": "Red Hat Enterprise Linux 7",
      "fix_state": "Not affected",
      "package_name": "kernel",
      "cpe": "cpe:/o:redhat:enterprise_linux:7"
    },
    {
      "product_name": "Red Hat Enterprise Linux 7",
      "fix_state": "Not affected",
      "package_name": "kernel-rt",
      "cpe": "cpe:/o:redhat:enterprise_linux:7"
    },
    {
      "product_name": "Red Hat Enterprise Linux 8",
      "fix_state": "Will not fix",
      "package_name": "kernel",
      "cpe": "cpe:/o:redhat:enterprise_linux:8"
    },
    {
      "product_name": "Red Hat Enterprise Linux 8",
      "fix_state": "Will not fix",
      "package_name": "kernel-rt",
      "cpe": "cpe:/o:redhat:enterprise_linux:8"
    },
    {
      "product_name": "Red Hat Enterprise Linux 9",
      "fix_state": "Will not fix",
      "package_name": "kernel",
      "cpe": "cpe:/o:redhat:enterprise_linux:9"
    },
    {
      "product_name": "Red Hat Enterprise Linux 9",
      "fix_state": "Will not fix",
      "package_name": "kernel-rt",
      "cpe": "cpe:/o:redhat:enterprise_linux:9"
    }
  ],
  "threat_severity": "Moderate",
  "public_date": "2024-11-19T00:00:00Z",
  "bugzilla": {
    "description": "kernel: cgroup/bpf: use a dedicated workqueue for cgroup bpf destruction",
    "id": "2327364",
    "url": "https://bugzilla.redhat.com/show_bug.cgi?id=2327364"
  },
  "cvss": {
    "cvss_base_score": "",
    "cvss_scoring_vector": "",
    "status": ""
  },
  "cvss3": {
    "cvss3_base_score": "5.5",
    "cvss3_scoring_vector": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H",
    "status": "draft"
  },
  "iava": "",
  "cwe": "",
  "statement": "",
  "acknowledgement": "",
  "name": "CVE-2024-53054",
  "document_distribution": "",
  "details": [
    "In the Linux kernel, the following vulnerability has been resolved:\ncgroup/bpf: use a dedicated workqueue for cgroup bpf destruction\nA hung_task problem shown below was found:\nINFO: task kworker/0:0:8 blocked for more than 327 seconds.\n\"echo 0 \u003e /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\nWorkqueue: events cgroup_bpf_release\nCall Trace:\n\u003cTASK\u003e\n__schedule+0x5a2/0x2050\n? find_held_lock+0x33/0x100\n? wq_worker_sleeping+0x9e/0xe0\nschedule+0x9f/0x180\nschedule_preempt_disabled+0x25/0x50\n__mutex_lock+0x512/0x740\n? cgroup_bpf_release+0x1e/0x4d0\n? cgroup_bpf_release+0xcf/0x4d0\n? process_scheduled_works+0x161/0x8a0\n? cgroup_bpf_release+0x1e/0x4d0\n? mutex_lock_nested+0x2b/0x40\n? __pfx_delay_tsc+0x10/0x10\nmutex_lock_nested+0x2b/0x40\ncgroup_bpf_release+0xcf/0x4d0\n? process_scheduled_works+0x161/0x8a0\n? trace_event_raw_event_workqueue_execute_start+0x64/0xd0\n? process_scheduled_works+0x161/0x8a0\nprocess_scheduled_works+0x23a/0x8a0\nworker_thread+0x231/0x5b0\n? __pfx_worker_thread+0x10/0x10\nkthread+0x14d/0x1c0\n? __pfx_kthread+0x10/0x10\nret_from_fork+0x59/0x70\n? __pfx_kthread+0x10/0x10\nret_from_fork_asm+0x1b/0x30\n\u003c/TASK\u003e\nThis issue can be reproduced by the following pressuse test:\n1. A large number of cpuset cgroups are deleted.\n2. Set cpu on and off repeatly.\n3. Set watchdog_thresh repeatly.\nThe scripts can be obtained at LINK mentioned above the signature.\nThe reason for this issue is cgroup_mutex and cpu_hotplug_lock are\nacquired in different tasks, which may lead to deadlock.\nIt can lead to a deadlock through the following steps:\n1. A large number of cpusets are deleted asynchronously, which puts a\nlarge number of cgroup_bpf_release works into system_wq. The max_active\nof system_wq is WQ_DFL_ACTIVE(256). Consequently, all active works are\ncgroup_bpf_release works, and many cgroup_bpf_release works will be put\ninto inactive queue. As illustrated in the diagram, there are 256 (in\nthe acvtive queue) + n (in the inactive queue) works.\n2. Setting watchdog_thresh will hold cpu_hotplug_lock.read and put\nsmp_call_on_cpu work into system_wq. However step 1 has already filled\nsystem_wq, 'sscs.work' is put into inactive queue. 'sscs.work' has\nto wait until the works that were put into the inacvtive queue earlier\nhave executed (n cgroup_bpf_release), so it will be blocked for a while.\n3. Cpu offline requires cpu_hotplug_lock.write, which is blocked by step 2.\n4. Cpusets that were deleted at step 1 put cgroup_release works into\ncgroup_destroy_wq. They are competing to get cgroup_mutex all the time.\nWhen cgroup_metux is acqured by work at css_killed_work_fn, it will\ncall cpuset_css_offline, which needs to acqure cpu_hotplug_lock.read.\nHowever, cpuset_css_offline will be blocked for step 3.\n5. At this moment, there are 256 works in active queue that are\ncgroup_bpf_release, they are attempting to acquire cgroup_mutex, and as\na result, all of them are blocked. Consequently, sscs.work can not be\nexecuted. Ultimately, this situation leads to four processes being\nblocked, forming a deadlock.\nsystem_wq(step1)WatchDog(step2)cpu offline(step3)cgroup_destroy_wq(step4)\n...\n2000+ cgroups deleted asyn\n256 actives + n inactives\n__lockup_detector_reconfigure\nP(cpu_hotplug_lock.read)\nput sscs.work into system_wq\n256 + n + 1(sscs.work)\nsscs.work wait to be executed\nwarting sscs.work finish\npercpu_down_write\nP(cpu_hotplug_lock.write)\n...blocking...\ncss_killed_work_fn\nP(cgroup_mutex)\ncpuset_css_offline\nP(cpu_hotplug_lock.read)\n...blocking...\n256 cgroup_bpf_release\nmutex_lock(\u0026cgroup_mutex);\n..blocking...\nTo fix the problem, place cgroup_bpf_release works on a dedicated\nworkqueue which can break the loop and solve the problem. System wqs are\nfor misc things which shouldn't create a large number of concurrent work\nitems. If something is going to generate \u003e\n---truncated---"
  ],
  "references": [
    "https://www.cve.org/CVERecord?id=CVE-2024-53054\nhttps://nvd.nist.gov/vuln/detail/CVE-2024-53054\nhttps://lore.kernel.org/linux-cve-announce/2024111927-CVE-2024-53054-7da4@gregkh/T"
  ]
}
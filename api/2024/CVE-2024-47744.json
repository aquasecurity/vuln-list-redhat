{
  "affected_release": null,
  "package_state": [
    {
      "product_name": "Red Hat Enterprise Linux 6",
      "fix_state": "Not affected",
      "package_name": "kernel",
      "cpe": "cpe:/o:redhat:enterprise_linux:6"
    },
    {
      "product_name": "Red Hat Enterprise Linux 7",
      "fix_state": "Not affected",
      "package_name": "kernel",
      "cpe": "cpe:/o:redhat:enterprise_linux:7"
    },
    {
      "product_name": "Red Hat Enterprise Linux 7",
      "fix_state": "Not affected",
      "package_name": "kernel-rt",
      "cpe": "cpe:/o:redhat:enterprise_linux:7"
    },
    {
      "product_name": "Red Hat Enterprise Linux 8",
      "fix_state": "Not affected",
      "package_name": "kernel",
      "cpe": "cpe:/o:redhat:enterprise_linux:8"
    },
    {
      "product_name": "Red Hat Enterprise Linux 8",
      "fix_state": "Not affected",
      "package_name": "kernel-rt",
      "cpe": "cpe:/o:redhat:enterprise_linux:8"
    },
    {
      "product_name": "Red Hat Enterprise Linux 9",
      "fix_state": "Not affected",
      "package_name": "kernel",
      "cpe": "cpe:/o:redhat:enterprise_linux:9"
    },
    {
      "product_name": "Red Hat Enterprise Linux 9",
      "fix_state": "Not affected",
      "package_name": "kernel-rt",
      "cpe": "cpe:/o:redhat:enterprise_linux:9"
    }
  ],
  "threat_severity": "Moderate",
  "public_date": "2024-10-21T00:00:00Z",
  "bugzilla": {
    "description": "kernel: KVM: Use dedicated mutex to protect kvm_usage_count to avoid deadlock",
    "id": "2320205",
    "url": "https://bugzilla.redhat.com/show_bug.cgi?id=2320205"
  },
  "cvss": {
    "cvss_base_score": "",
    "cvss_scoring_vector": "",
    "status": ""
  },
  "cvss3": {
    "cvss3_base_score": "5.5",
    "cvss3_scoring_vector": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H",
    "status": "draft"
  },
  "iava": "",
  "cwe": "CWE-667",
  "statement": "",
  "acknowledgement": "",
  "name": "CVE-2024-47744",
  "document_distribution": "",
  "details": [
    "In the Linux kernel, the following vulnerability has been resolved:\nKVM: Use dedicated mutex to protect kvm_usage_count to avoid deadlock\nUse a dedicated mutex to guard kvm_usage_count to fix a potential deadlock\non x86 due to a chain of locks and SRCU synchronizations.  Translating the\nbelow lockdep splat, CPU1 #6 will wait on CPU0 #1, CPU0 #8 will wait on\nCPU2 #3, and CPU2 #7 will wait on CPU1 #4 (if there's a writer, due to the\nfairness of r/w semaphores).\nCPU0                     CPU1                     CPU2\n1   lock(\u0026kvm-\u003eslots_lock);\n2                                                     lock(\u0026vcpu-\u003emutex);\n3                                                     lock(\u0026kvm-\u003esrcu);\n4                            lock(cpu_hotplug_lock);\n5                            lock(kvm_lock);\n6                            lock(\u0026kvm-\u003eslots_lock);\n7                                                     lock(cpu_hotplug_lock);\n8   sync(\u0026kvm-\u003esrcu);\nNote, there are likely more potential deadlocks in KVM x86, e.g. the same\npattern of taking cpu_hotplug_lock outside of kvm_lock likely exists with\n__kvmclock_cpufreq_notifier():\ncpuhp_cpufreq_online()\n|\n-\u003e cpufreq_online()\n|\n-\u003e cpufreq_gov_performance_limits()\n|\n-\u003e __cpufreq_driver_target()\n|\n-\u003e __target_index()\n|\n-\u003e cpufreq_freq_transition_begin()\n|\n-\u003e cpufreq_notify_transition()\n|\n-\u003e ... __kvmclock_cpufreq_notifier()\nBut, actually triggering such deadlocks is beyond rare due to the\ncombination of dependencies and timings involved.  E.g. the cpufreq\nnotifier is only used on older CPUs without a constant TSC, mucking with\nthe NX hugepage mitigation while VMs are running is very uncommon, and\ndoing so while also onlining/offlining a CPU (necessary to generate\ncontention on cpu_hotplug_lock) would be even more unusual.\nThe most robust solution to the general cpu_hotplug_lock issue is likely\nto switch vm_list to be an RCU-protected list, e.g. so that x86's cpufreq\nnotifier doesn't to take kvm_lock.  For now, settle for fixing the most\nblatant deadlock, as switching to an RCU-protected list is a much more\ninvolved change, but add a comment in locking.rst to call out that care\nneeds to be taken when walking holding kvm_lock and walking vm_list.\n======================================================\nWARNING: possible circular locking dependency detected\n6.10.0-smp--c257535a0c9d-pip #330 Tainted: G S         O\n------------------------------------------------------\ntee/35048 is trying to acquire lock:\nff6a80eced71e0a8 (\u0026kvm-\u003eslots_lock){+.+.}-{3:3}, at: set_nx_huge_pages+0x179/0x1e0 [kvm]\nbut task is already holding lock:\nffffffffc07abb08 (kvm_lock){+.+.}-{3:3}, at: set_nx_huge_pages+0x14a/0x1e0 [kvm]\nwhich lock already depends on the new lock.\nthe existing dependency chain (in reverse order) is:\n-\u003e #3 (kvm_lock){+.+.}-{3:3}:\n__mutex_lock+0x6a/0xb40\nmutex_lock_nested+0x1f/0x30\nkvm_dev_ioctl+0x4fb/0xe50 [kvm]\n__se_sys_ioctl+0x7b/0xd0\n__x64_sys_ioctl+0x21/0x30\nx64_sys_call+0x15d0/0x2e60\ndo_syscall_64+0x83/0x160\nentry_SYSCALL_64_after_hwframe+0x76/0x7e\n-\u003e #2 (cpu_hotplug_lock){++++}-{0:0}:\ncpus_read_lock+0x2e/0xb0\nstatic_key_slow_inc+0x16/0x30\nkvm_lapic_set_base+0x6a/0x1c0 [kvm]\nkvm_set_apic_base+0x8f/0xe0 [kvm]\nkvm_set_msr_common+0x9ae/0xf80 [kvm]\nvmx_set_msr+0xa54/0xbe0 [kvm_intel]\n__kvm_set_msr+0xb6/0x1a0 [kvm]\nkvm_arch_vcpu_ioctl+0xeca/0x10c0 [kvm]\nkvm_vcpu_ioctl+0x485/0x5b0 [kvm]\n__se_sys_ioctl+0x7b/0xd0\n__x64_sys_ioctl+0x21/0x30\nx64_sys_call+0x15d0/0x2e60\ndo_syscall_64+0x83/0x160\nentry_SYSCALL_64_after_hwframe+0x76/0x7e\n-\u003e #1 (\u0026kvm-\u003esrcu){.+.+}-{0:0}:\n__synchronize_srcu+0x44/0x1a0\n---truncated---"
  ],
  "references": [
    "https://www.cve.org/CVERecord?id=CVE-2024-47744\nhttps://nvd.nist.gov/vuln/detail/CVE-2024-47744\nhttps://lore.kernel.org/linux-cve-announce/2024102111-CVE-2024-47744-7571@gregkh/T"
  ]
}
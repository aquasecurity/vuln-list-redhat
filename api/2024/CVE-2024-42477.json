{
  "affected_release": null,
  "package_state": null,
  "threat_severity": "Moderate",
  "public_date": "2024-08-12T00:00:00Z",
  "bugzilla": {
    "description": "llama-cpp: global-buffer-overflow in ggml_type_size",
    "id": "2304435",
    "url": "https://bugzilla.redhat.com/show_bug.cgi?id=2304435"
  },
  "cvss": {
    "cvss_base_score": "",
    "cvss_scoring_vector": "",
    "status": ""
  },
  "cvss3": {
    "cvss3_base_score": "",
    "cvss3_scoring_vector": "",
    "status": ""
  },
  "iava": "",
  "cwe": "CWE-125",
  "statement": "",
  "acknowledgement": "",
  "name": "CVE-2024-42477",
  "document_distribution": "",
  "details": [
    "llama.cpp provides LLM inference in C/C++. The unsafe `type` member in the `rpc_tensor` structure can cause `global-buffer-overflow`. This vulnerability may lead to memory data leakage. The vulnerability is fixed in b3561."
  ],
  "references": [
    "https://www.cve.org/CVERecord?id=CVE-2024-42477\nhttps://nvd.nist.gov/vuln/detail/CVE-2024-42477\nhttps://github.com/ggerganov/llama.cpp/commit/b72942fac998672a79a1ae3c03b340f7e629980b\nhttps://github.com/ggerganov/llama.cpp/security/advisories/GHSA-mqp6-7pv6-fqjf"
  ]
}
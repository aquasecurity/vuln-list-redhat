{
  "affected_release": null,
  "package_state": [
    {
      "product_name": "Red Hat Enterprise Linux 6",
      "fix_state": "Out of support scope",
      "package_name": "kernel",
      "cpe": "cpe:/o:redhat:enterprise_linux:6"
    },
    {
      "product_name": "Red Hat Enterprise Linux 7",
      "fix_state": "Out of support scope",
      "package_name": "kernel",
      "cpe": "cpe:/o:redhat:enterprise_linux:7"
    },
    {
      "product_name": "Red Hat Enterprise Linux 7",
      "fix_state": "Out of support scope",
      "package_name": "kernel-rt",
      "cpe": "cpe:/o:redhat:enterprise_linux:7"
    },
    {
      "product_name": "Red Hat Enterprise Linux 8",
      "fix_state": "Not affected",
      "package_name": "kernel",
      "cpe": "cpe:/o:redhat:enterprise_linux:8"
    },
    {
      "product_name": "Red Hat Enterprise Linux 8",
      "fix_state": "Not affected",
      "package_name": "kernel-rt",
      "cpe": "cpe:/o:redhat:enterprise_linux:8"
    },
    {
      "product_name": "Red Hat Enterprise Linux 9",
      "fix_state": "Not affected",
      "package_name": "kernel",
      "cpe": "cpe:/o:redhat:enterprise_linux:9"
    },
    {
      "product_name": "Red Hat Enterprise Linux 9",
      "fix_state": "Not affected",
      "package_name": "kernel-rt",
      "cpe": "cpe:/o:redhat:enterprise_linux:9"
    }
  ],
  "threat_severity": "Low",
  "public_date": "2025-02-26T00:00:00Z",
  "bugzilla": {
    "description": "kernel: btrfs: fix deadlock between concurrent dio writes when low on free data space",
    "id": "2348270",
    "url": "https://bugzilla.redhat.com/show_bug.cgi?id=2348270"
  },
  "cvss": {
    "cvss_base_score": "",
    "cvss_scoring_vector": "",
    "status": ""
  },
  "cvss3": {
    "cvss3_base_score": "5.5",
    "cvss3_scoring_vector": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H",
    "status": "draft"
  },
  "iava": "",
  "cwe": "",
  "statement": "",
  "acknowledgement": "",
  "name": "CVE-2022-49547",
  "document_distribution": "",
  "details": [
    "In the Linux kernel, the following vulnerability has been resolved:\nbtrfs: fix deadlock between concurrent dio writes when low on free data space\nWhen reserving data space for a direct IO write we can end up deadlocking\nif we have multiple tasks attempting a write to the same file range, there\nare multiple extents covered by that file range, we are low on available\nspace for data and the writes don't expand the inode's i_size.\nThe deadlock can happen like this:\n1) We have a file with an i_size of 1M, at offset 0 it has an extent with\na size of 128K and at offset 128K it has another extent also with a\nsize of 128K;\n2) Task A does a direct IO write against file range [0, 256K), and because\nthe write is within the i_size boundary, it takes the inode's lock (VFS\nlevel) in shared mode;\n3) Task A locks the file range [0, 256K) at btrfs_dio_iomap_begin(), and\nthen gets the extent map for the extent covering the range [0, 128K).\nAt btrfs_get_blocks_direct_write(), it creates an ordered extent for\nthat file range ([0, 128K));\n4) Before returning from btrfs_dio_iomap_begin(), it unlocks the file\nrange [0, 256K);\n5) Task A executes btrfs_dio_iomap_begin() again, this time for the file\nrange [128K, 256K), and locks the file range [128K, 256K);\n6) Task B starts a direct IO write against file range [0, 256K) as well.\nIt also locks the inode in shared mode, as it's within the i_size limit,\nand then tries to lock file range [0, 256K). It is able to lock the\nsubrange [0, 128K) but then blocks waiting for the range [128K, 256K),\nas it is currently locked by task A;\n7) Task A enters btrfs_get_blocks_direct_write() and tries to reserve data\nspace. Because we are low on available free space, it triggers the\nasync data reclaim task, and waits for it to reserve data space;\n8) The async reclaim task decides to wait for all existing ordered extents\nto complete (through btrfs_wait_ordered_roots()).\nIt finds the ordered extent previously created by task A for the file\nrange [0, 128K) and waits for it to complete;\n9) The ordered extent for the file range [0, 128K) can not complete\nbecause it blocks at btrfs_finish_ordered_io() when trying to lock the\nfile range [0, 128K).\nThis results in a deadlock, because:\n- task B is holding the file range [0, 128K) locked, waiting for the\nrange [128K, 256K) to be unlocked by task A;\n- task A is holding the file range [128K, 256K) locked and it's waiting\nfor the async data reclaim task to satisfy its space reservation\nrequest;\n- the async data reclaim task is waiting for ordered extent [0, 128K)\nto complete, but the ordered extent can not complete because the\nfile range [0, 128K) is currently locked by task B, which is waiting\non task A to unlock file range [128K, 256K) and task A waiting\non the async data reclaim task.\nThis results in a deadlock between 4 task: task A, task B, the async\ndata reclaim task and the task doing ordered extent completion (a work\nqueue task).\nThis type of deadlock can sporadically be triggered by the test case\ngeneric/300 from fstests, and results in a stack trace like the following:\n[12084.033689] INFO: task kworker/u16:7:123749 blocked for more than 241 seconds.\n[12084.034877]       Not tainted 5.18.0-rc2-btrfs-next-115 #1\n[12084.035562] \"echo 0 \u003e /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\n[12084.036548] task:kworker/u16:7   state:D stack:    0 pid:123749 ppid:     2 flags:0x00004000\n[12084.036554] Workqueue: btrfs-flush_delalloc btrfs_work_helper [btrfs]\n[12084.036599] Call Trace:\n[12084.036601]  \u003cTASK\u003e\n[12084.036606]  __schedule+0x3cb/0xed0\n[12084.036616]  schedule+0x4e/0xb0\n[12084.036620]  btrfs_start_ordered_extent+0x109/0x1c0 [btrfs]\n[12084.036651]  ? prepare_to_wait_exclusive+0xc0/0xc0\n[12084.036659]  btrfs_run_ordered_extent_work+0x1a/0x30 [btrfs]\n[12084.036688]  btrfs_work_helper+0xf8/0x400 [btrfs]\n[12084.0367\n---truncated---"
  ],
  "references": [
    "https://www.cve.org/CVERecord?id=CVE-2022-49547\nhttps://nvd.nist.gov/vuln/detail/CVE-2022-49547\nhttps://lore.kernel.org/linux-cve-announce/2025022615-CVE-2022-49547-c0fc@gregkh/T"
  ]
}